---
layout: post
title: "High Performance Computing Notes"
date: 2020-02-01
excerpt: "HPC learning notes, MPI etc. etc. updating"
tags: [C, HPC, MPI]
comments: true
---

## Install MPICH on local machines:
For my ubuntu and mac:<br>
1. Install MPICH from the tar ball: [mpich-3.3.2](https://www.mpich.org/downloads/)
```
$ wget http://www.mpich.org/static/downloads/3.3.2/mpich-3.3.2.tar.gz
$ tar -xzf mpich-3.3.2.tar.gz
$ cd mpich-3.3.2
```
2. Configure, avoid building fortran library as not needed:
```
$ ./configure --disable-fortran
```
    ... Wait until the command logs showing Configuration completed.
  
3. Make and install:
```
$ make; sudo make install
```
4. test with `$ mpiexec --version`, if make is successful, call this command, will have the following:
```
HYDRA build details:
    Version:                                 3.3.2
    Release Date:                            Tue Nov 12 21:23:16 CST 2019
    CC:                              gcc    
    CXX:                             g++    
    F77:                             
    F90:                             
    Configure options:                       '--disable-option-checking' '--prefix=NONE' '--disable-fortran' '--cache-file=/dev/null' '--srcdir=.' 'CC=gcc' 'CFLAGS= -O2' 'LDFLAGS=' 'LIBS=' 'CPPFLAGS= -I/home/muyangguo/mpich-3.3.2/src/mpl/include -I/home/muyangguo/mpich-3.3.2/src/mpl/include -I/home/muyangguo/mpich-3.3.2/src/openpa/src -I/home/muyangguo/mpich-3.3.2/src/openpa/src -D_REENTRANT -I/home/muyangguo/mpich-3.3.2/src/mpi/romio/include' 'MPLLIBNAME=mpl'
    Process Manager:                         pmi
    Launchers available:                     ssh rsh fork slurm ll lsf sge manual persist
    Topology libraries available:            hwloc
    Resource management kernels available:   user slurm ll lsf sge pbs cobalt
    Checkpointing libraries available:       
    Demux engines available:                 poll select
```


The cluster used MPI lib is mvapich2, I chose to use MPICH on my local machine, as many features for clusters and networks are not needed. Alternatively, another open source distribution is OpenMPI. I just go with MPICH, either one should work. 


## 1. A Sum algorithm to start:

This is a programming assignment from my 6220 class, to perform a sum with MPI.

The code is archived [here](https://github.com/MUYANGGUO/CSE6220-HPC/blob/master/projects/project-1/prog1.cpp)

<div  style="overflow:scroll; height: 500px;">
{% highlight c %}
#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <math.h>

#define MPI_CHK(err) if (err != MPI_SUCCESS) return err

double local_sum(long int local_n,long int local_c,double local_nums[],double* p);

int main(int argc, char *argv[]) {
    if(argc!=3){
        fprintf(stderr,"argc invalid, must have N, C specified. aborting the program ... \n");
        exit(EXIT_FAILURE);
    }
    int rank,size;
    // Initialize the MPI environment
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);
    int p = size;
    int err;
    const int root = 0;
    long int arg_buff[2];
    if (rank == root){
        arg_buff[0]= atoi(argv[1]);
        arg_buff[1] = atoi(argv[2]);
    }
    // all ranks start calling MPI_Bcast, root to all other ranks, assign a buff to send the argv, buff = [n,c], already converted to int type
    MPI_Bcast(&arg_buff, 2, MPI_DOUBLE, root, MPI_COMM_WORLD);
    //start the timer
    double t_start = MPI_Wtime();
    //after Bcast, cast values to variable n, c , and calculate each rank's new seed number 
    long int n;
    long int c;
    n = arg_buff[0];
    c = arg_buff[1];
    long int local_c;
    local_c = c+rank;
    // printf("[%d]: After Bcast, number size is %li, new seed number is %li\n",rank, n, local_c);
    int d = log2(p);
    //printf("[%d]: is having %d dimensions \n",rank,d);
    // consider the N/P, N is not divisible by P cases, we need to assign the maximum numbers for a rank, so some may have one more number than the other.
    int local_n;
    int remainder = n%size;
    if (remainder != 0 && remainder >= rank+1){
        local_n = n/size + 1;
        //printf("[%d]: assigned %d size numbers to this processor\n",rank, local_n);
    }
    else{
        local_n = n/size;
        //printf("[%d]: assigned %d size numbers to this processor\n",rank, local_n);
    }
    // after calculated the local_c, the seed for each processor, and the local_n, the number size     //assigned to each processor. 
    // we need to generate the random numbers based on the local_c, local_n;
    //specify the request array size, based on local_n assigned to each processor
    double local_nums[local_n+1];
    //malloc the memory needed
    double* ptr = (double *) malloc ( sizeof(double) * local_n );
    if (ptr == NULL){
        printf("[%d]: Error! memory not allocated.\n",rank);
        exit(EXIT_FAILURE);
    }
    double local_s = local_sum(local_n,local_c,local_nums,ptr);
    free(ptr);
    ptr = NULL;
    //start send local sum in pairs 
    for( int j= 0; j < d; j++ ){
        int bit = pow(2, j);
        if ((rank & bit) !=0){
            // printf("j = %d with rank %d, send to %d\n",j,rank,(rank ^ bit));
            MPI_Send(&local_s, 1, MPI_DOUBLE,(rank ^ bit), 111, MPI_COMM_WORLD);
            MPI_Finalize();
            return 0;
        }
        else{
            MPI_Status stat;
            double local_s_received;
            MPI_Recv (&local_s_received, 1, MPI_DOUBLE, (rank ^ bit), 111, MPI_COMM_WORLD, &stat);
            local_s = local_s+local_s_received;
        }
    } 
    if (rank==0){
    double sum = local_s;
    double t_end = MPI_Wtime();
    double t_running = t_end - t_start;
    printf("sum is: %f",sum);
    FILE *f = fopen("output.txt", "a+");
    if (f == NULL)
    {
        printf("Error opening file!\n");
        exit(EXIT_FAILURE);
    }  
    fprintf(f, "N= %ld, P = %d, C= %ld, S= %f\nTime= %f\n", n,p,c,sum,t_running);
    fclose(f);
    }
    // Finalize the MPI environment.
    MPI_Finalize();
return 0;
}
double local_sum(long int local_n,long int local_c, double local_nums[],double* p){
    double local_s;
    srand48(local_c);
    int i;
    p = local_nums;
    for (i=0; i<local_n; i++){
        local_nums[i] = drand48();
        // printf(" generating:%f \n",local_nums[i]);
        local_s = local_s + local_nums[i];
    }
    return local_s;
}
{% endhighlight %}
</div>

---

```
mpicxx ./prog1.cpp -o prog1
mpirun -np 8 ./prog1 500 1
```
The algorithm is simple but very informative:
```
Algorithm (for P_i)
sum = add local N / P numbers
for j = 0 to d-1 {
if ((rank AND 2^j)!= 0) {
send sum to (rank XOR 2^j);
exit;
}
else {
receive sum' from (rank XOR 2^j);
sum = sum + sum';
}
}
if (rank == 0)
```
And here we use dimension d to determine which pairs of processors communicate with each other. And we use bitwise operator to XOR and AND to find the pairs for iterations. We could do that because the the sum should have log(P) iterations and we considered p is a power of 2. 

### some learning notes to pay attention for this example

1. MPI_Bcast is a MPI step all processors need to perform. 
2. For long int, we should use MPI_DOUBLE
3. Since it is log(P) iterations and sum goes to one last processor(root), we should exit the MPI for processors done their jobs during the iterations and won't need to communicate with others , by using MPI_Finalize() and return to exit the process.
4. for N sizes not dividable for P, we could allocate n and n+1 sizes by calculating the mod and compare to the rank nums. 

## 2. Parallel Prefix Sum:
Input: n numbers: $$x_0, x_1,...,x_n$$ <br>
Output: $$S_0, S_1, ... , S_n$$<br>
Math: $$S_i = \sum_{j=0}^{i}x_j$$<br>

#### Parallel Prefix Sum Alg-1

```
Algorithm-1 (for Pi)， suppose for n number, we have p=n processors

total_sum←prefix_sum←local_number
for j=0 to d-1 do 
	rank’ ←rank XOR 2^j
	send total_sum to rank’
	receive received_sum from rank’ 
	total_sum←total_sum+ received_sum
	if (rank > rank’)
		prefix_sum←prefix_sum+ received_sum
endfor
```
![Alg01](https://drive.google.com/uc?id=1CSurQ5WZjTEwmn7RJsNLDWut64s3ZUJu)
***here black color denote prefix sum, read denote total sum***<br>
However, if n>p_new, we have to use Brent's Lemma, to let each p_new_i simulate p/p_new_i's processors. <br>
So each p_new_i will have to assign a chunk of n numbers locally and perform the Alg-1.<br>
<figure>
	<img src="https://drive.google.com/uc?id=1TpoqaOvbKtq3hB6OVaz976oMPf2a9SxY" style="width:400px;height:300px;">
</figure>

#### Parallel Prefix Sum Alg-2
1.Compute prefix sum locally on each processor
2.Perform parallel prefix sum (Alg-1)*[1] using the last local prefix sum on each processor
3.Add the result of parallel prefix sum on a processor to each of its local prefix sum
![Alg02](https://drive.google.com/uc?id=1TeHDcuiIXvH_n407YZ7srMCXG7qJ-T7-)
***here black color denote prefix sum, read denote total sum*** <br>
*[1] We need to **Modify Alg-2 to start with prefix_sum←0** <br>
![Alg02-1](https://drive.google.com/uc?id=1w-pjFWgIwWjgyrCNBJlQ7uKiV4q7QA4A)

Note: <br>
- What if n is not divisible by p? <br>
Assign max $$\frac{n}{p}$$ to each processor: some processors will have 1 more element than the others.<br>
- What if p is not a power of 2?<br>
Find p'= a power of 2 such that  p'/2 < p < p'. Run the code like you have p' processors.Ignore communications to/from non-existing processors, i.e., rank >= p. 





---
Copyright 2019 Muyang Guo, all rights reserved. Redistribution of the work must cite the original source.
